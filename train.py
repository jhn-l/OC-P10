import os
import boto3
import pandas as pd
import pickle
import numpy as np
import scipy.sparse as sp
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler

# ğŸ“Œ Configurer AWS
S3_BUCKET_NAME = "my-recommender-dataset"
s3 = boto3.client("s3")
MODEL_PATH = "/tmp/recommender_model_hybrid.pkl"
CLICKS_PATH = "clicks/"

# ğŸ“Œ Charger un fichier depuis S3 en DataFrame
def load_csv_from_s3(file_key):
    obj = s3.get_object(Bucket=S3_BUCKET_NAME, Key=file_key)
    return pd.read_csv(obj["Body"])

# ğŸ“Œ Charger les interactions utilisateur-article depuis S3
def load_interactions():
    print("ğŸ”¹ Chargement des interactions utilisateur-article...")
    all_files = s3.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=CLICKS_PATH).get('Contents', [])
    df_list = [load_csv_from_s3(file["Key"]) for file in all_files if file["Key"].endswith(".csv")]
    interactions_df = pd.concat(df_list, ignore_index=True)
    
    interactions_df.rename(columns={"click_article_id": "article_id"}, inplace=True)
    interactions_df["article_id"] = interactions_df["article_id"].astype(int)
    
    print("âœ… Interactions chargÃ©es - Nombre de lignes:", interactions_df.shape[0])
    return interactions_df

# ğŸ“Œ Construire une matrice utilisateur-article **sparse**
def build_sparse_matrix(interactions_df):
    print("ğŸ”¹ Construction de la matrice utilisateur-article Ã©parse...")

    user_ids = interactions_df["user_id"].astype("category")
    article_ids = interactions_df["article_id"].astype("category")

    row = user_ids.cat.codes.values
    col = article_ids.cat.codes.values
    data = np.ones(len(interactions_df))

    sparse_matrix = sp.csr_matrix((data, (row, col)), shape=(user_ids.cat.categories.size, article_ids.cat.categories.size))
    
    print(f"âœ… Matrice utilisateur-article construite avec {sparse_matrix.nnz} interactions non nulles.")
    return sparse_matrix, user_ids, article_ids

# ğŸ“Œ SÃ©lection automatique de `n_components`
def choose_n_components(X, variance_threshold=0.95):
    svd = TruncatedSVD(n_components=min(300, X.shape[1] - 1))
    svd.fit(X)
    explained_variance = np.cumsum(svd.explained_variance_ratio_)

    optimal_n = np.argmax(explained_variance >= variance_threshold) + 1
    print(f"âœ… Nombre optimal de composantes SVD : {optimal_n} (Variance expliquÃ©e: {explained_variance[optimal_n-1]:.2f})")
    
    return optimal_n

# ğŸ“Œ EntraÃ®ner un modÃ¨le de filtrage collaboratif avec TruncatedSVD
def train_collaborative_model(interactions_df):
    print("ğŸ”¹ CrÃ©ation de la matrice utilisateur-article...")
    sparse_matrix, user_ids, article_ids = build_sparse_matrix(interactions_df)

    # ğŸ”¹ RÃ©duction de dimension avec TruncatedSVD
    print("ğŸ”¹ EntraÃ®nement du modÃ¨le SVD...")
    n_components = choose_n_components(sparse_matrix)

    svd = TruncatedSVD(n_components=n_components)
    svd.fit(sparse_matrix)

    print(f"âœ… Sauvegarde du modÃ¨le localement dans {MODEL_PATH}...")
    with open(MODEL_PATH, "wb") as f:
        pickle.dump((svd, user_ids, article_ids), f)  # Stocker les index pour l'infÃ©rence

    upload_model_to_s3()
    print("ğŸš€ ModÃ¨le entraÃ®nÃ© et sauvegardÃ© avec succÃ¨s sur S3 !")

# ğŸ“Œ Upload du modÃ¨le vers S3
def upload_model_to_s3():
    print(f"ğŸš€ Upload du modÃ¨le vers S3: {S3_BUCKET_NAME}/recommender_model_hybrid.pkl...")
    s3.upload_file(MODEL_PATH, S3_BUCKET_NAME, "recommender_model_hybrid.pkl")
    print("âœ… ModÃ¨le uploadÃ© avec succÃ¨s sur S3 !")

# ğŸ“Œ ExÃ©cution du script d'entraÃ®nement
if __name__ == "__main__":
    interactions_df = load_interactions()
    train_collaborative_model(interactions_df)
